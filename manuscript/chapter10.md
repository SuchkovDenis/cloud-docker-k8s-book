# 10. Управление ресурсами Kubernetes

До этого момента мы проводили развертывания своих микросервисов в кластере Kubernetes так, как если бы они обладали бесконечным количеством вычислительной мощности и памяти. Для наших простейших примеров даже миниатюрные локальные кластеры `minikube` и `kind` способны запустить десятки экземпляров наших микроскопических сервисов, однако все сразу же меняется в реальных условиях и развертывании настоящих приложений. 

Кластер Kubernetes, как мы узнали еще в самом начале знакомства с ним, состоит из узлов (nodes), чаще всего виртуальных (реже реальных) машин Linux. Сложив мощность процессоров и памяти этих узлов, мы получим все ресурсы процессора и памяти, доступные для развертывания своих приложений. Однако развертывать мы будем больше не монолит, а систему микросервисов. Сразу же возникает вопрос разделения доступных ресурсов - какие микросервисы получат больше вычислительной мощности процессора и памяти? К примеру, что если микросервису Java на основе Spring Boot не хватит памяти в куче при пиковой загрузке, потому что оставшаяся память на этом же узле будет занята десятком микросервисов на Go? Качество обслуживания запросов резко упадет, а пользователи быстро разочаруются.

В динамическом мире Kubernetes, где сосуществуют десятки (а иногда и сотни и тысячи, как в компаниях Uber или Twitter) микросервисов, мы не можем полагаться на то, что каждый из них чудесным образом будет делиться ресурсами. В идеале, каждый микросервис и компонент системы должен описать, сколько их ему необходимо. 

Управляющие системы Kubernetes же выступают в роли *планировщика, или диспетчера задач* (*task scheduler*), выясняя на каких узлах системы достаточно ресурсов, как эффективно они используются, и какой узел наилучшим образом подойдет на запуска нового контейнера. Планирование задач является фундаментальной частью операционных систем, и Kubernetes по сути выступает здесь как операционная система для наших микросервисов в облаке.

## Распределение ресурсов по умолчанию

Если требования микросервисов к ресурсам не указаны явно, управление Kubernetes разделит доступные на узле процессор и память примерно поровну, но только в начале работы новых контейнеров (в отсеках pod). Даже в наших простых примерах очевидно, что ресурсов микросервис `time-service`, написанный на Go, будет потреблять намного меньше, чем более сложный, написанный на Java `weekend-service`. Если мы решим увеличить количество экземпляров, или столкнемся с пиковой нагрузкой на сервисы, рано или поздно ресурсов станет не хватать, и именно ресурсы, занятые `time-service` в примерно одинаковой пропорции, будут простаивать впустую. Как минимум, возникает проблема простоя ресурсов, все более серьезная при увеличении нагрузки на систему - единственным выходом станет добавление мощности в кластер в виде более мощных узлов (node), или большего их количества - а это дорого и неэффективно.

Вторая проблема может стать причиной отказа системы. Как мы сказали, ресурсы разделяются примерно поровну только в начале работы. Не совсем удачная версия или обновление может привести к тому, что микросервис станет потреблять больше вычислительной мощности процессора, или будет выпущен с утечкой памяти. Так как верхней границы у потребления ресурсов нет, постепенно такой микросервис может занять большую часть памяти и процессора, и привести к медленной работе остальных микросервисов, работающих на том же узле кластера. Недостаток памяти тут же приведет к тому, что на таком узле станет невозможно запустить требовательные к ресурсам компоненты системы, например большой Java-сервис.

Именно поэтому хорошей практикой работы в Kubernetes является определение требований к ресурсам ко всем микросервисам системы. Если в процессе разработки и тестирования, в условиях мощного кластера, ими еще можно пренебречь, в эксплуатации реальной системы они совершенно необходимы.

## Требования к ресурсам в развертываниях Deployment

Как и все, что касается развертывания и запуска микросервисов в кластере Kubernetes, требования к ресурсам будут описаны в объекте Deployment. Именно там мы описываем образы контейнеров для запуска отсеков Pod, количество их экземпляров, проверки готовности и жизнеспобности, логично что именно там же мы объявим какое количество ресурсов понадобится для работы нашего микросервиса (или просто какого-то компонента системы).

Давайте опишем минимально необходимое количество ресурсов для нашего сервиса выходного дня `weekend-service` - он написан на Java, и естественным образом потребует некоторого количества памяти для виртуальной машины и “кучи” (heap), и чуть большей вычислительной мощности, чем миниатюрный `time-service`, написанный на Go. Ресурсы объявляются в подсекции `resources`, там же где проверки готовности. Для простоты создадим отдельный файл Deployment в папке `weekend-service/k8s/resources`:

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  labels:
    app: weekend-service
  name: weekend-service
…
    spec:
      containers:
      - name: weekend-service
        image: ivanporty/weekend-service:0.2.0
        readinessProbe:
        … 
       resources:
          requests:
            memory: "64M"
            cpu: "1"

```

Мы объявляем, что нашему микросервису понадобятся (секция `requests`) как минимум 64 мегабайта памяти (`memory`) - довольно большое количество для миниатюрного сервиса, но виртуальная машина Java известна своими аппетитами, и один процессор (`cpu`). Ресурсы в Kubernetes можно описывать с довольно большой точностью:

* Процессор (`cpu`) - мы указали один “процессор”, это означает, что *в целом* наше приложение будет использовать вычислительную мощность *одного ядра* процессора, того типа и мощности, что использован на узлах (node) нашего кластера. Это может быть или ядро реального процессора (если узел является выделенным сервером), или виртуального (для виртуальных машин, то есть для большинства кластеров в облаке). В целом значит то, что выделять и фиксировать за нами ядро процессора вряд ли станут - в целом доступная мощность будет соответствовать одному ядру, а распределяться наши задачи будут по всем доступным ядрам, в зависимости от алгоритма Linux. Минимальное количество процессора - одна его милли-часть, то есть 0.001 от целого ядра, обозначаемая маленькой `m` (`cpu: "1m"`)
* Память (`memory`) - тут все интереснее, потому что указывать требуемую память можно в очень большом количестве вариантов и единиц. Мы указали мегабайты (`M`) - это десятичная система, то есть миллион байт. Есть еще мебибайты (`Mi`), в двоичной системе (1024^2), то же самое относится к кило- и киби, гига- и гиби. Если указать простое число, это будут просто байты. А вот есть сказать `64m`, это будет 64 милли-байта, сомнительно что мы найдем микросервис способный работать на таком количестве памяти, так что используйте правильный регистр.

Удостоверимся теперь, что ресурсы, запрошенные нами, доступны, и описаны верно. Проведем обновленное развертывание (если в вашем кластере не запущен сервис weekend-service, разверните его в дополнение к обновленному развертыванию):

```console
$ kubectl apply -f k8s/resources/
deployment.apps/weekend-service created
# если в кластере еще не был развернут weekend-service
$ kubectl apply -f k8s/k8s-weekend-svc.yaml
service/weekend-service created

```

Как видно, Kubernetes принял наше обновленное развертывание. В качестве простого упражнения остается переадресовать порт сервиса, и проверить что сервис выходного дня работает (а еще ему понадобится работающий сервис-партнер `time-service` - разверните его в кластере, если он там не остался после предыдущих экспериментов).

Поддержка метрик. Проверка ресурсов, minikube dashboard.

Мы определили минимально необходимые нам ресурсы (`resource.requests`), однако для стабильной безотказной работы кластера важнее определить верхнюю границу используемых ресурсов (`limit`). Мы уже обсудили, как неудачная версия сервиса с утечкой памяти может внести хаос в ресурсы узла, и ограничение ресурсов позволяет этого избежать. Верхние границы ресурсов определяются примерно так же:

```yaml
…
        resources:
          requests:
            memory: "64M"
            cpu: "1"
          limits:
            memory: "256M"
            cpu: "2"
…
```

Заново разверните обновленный объект Deployment, и теперь наш микросервис `weekend-service` даже при всем желании не сможет превысить 256 мегабайт памяти и *в общем* 2 ядра процессора. Дело в том, что при работе код сервиса может превысить лимит процессора, но только в момент непосредственного исполнения на процессоре. После его паузы управление Kubernetes и среда запуска контейнеров поймет, что процесс превышает верхнюю границу загрузки и назначит ему меньший приоритет, то есть станет давать меньше времени процессора.

С памятью же это жесткий лимит, и при попытке использовать больше памяти процесс получит ошибку от операционной системы и будет перезапущен. Это легко проверить - уменьшите лимит памяти до 128 мегабайт, и вы увидите что `weekend-service` не сможет запуститься - он постоянно будет завершаться с ошибкой `OOMKilled` - процесс будет пытаться получить больше памяти чем он запросил.

Назначив верхние лимиты памяти и процессора, и запросив минимально необходимые ресурсы (часто они совпадают, в этом случае можно описать просто лимит `resources.limits`), мы сможем обезопасить себя от истощения ресурсов, и позволить управлению Kubernetes максимально эффективно использовать все ресурсы кластера.


## Резюме

